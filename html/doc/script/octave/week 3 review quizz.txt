
Suppose you have the following training set, and fit a logistic regression classifier hθ(x)=g(θ0+θ1x1+θ2x2).
*  J(θ) will be a convex function, so gradient descent should converge to the global minimum.  (They mean a properly formulated J(p) cost function.)
f  The positive and negative examples cannot be separated using a straight line. So, gradient descent will fail to converge.

For logistic regression, the gradient is given by ∂∂θjJ(θ)=1m∑mi=1(hθ(x(i))−y(i))x(i)j. 
Which of these is a correct gradient descent update for logistic regression with a learning rate of α? Check all that apply.
  θ:=θ−α1m∑mi=1(θTx−y(i))x(i).
*  θ:=θ−α1m∑mi=1(hθ(x(i))−y(i))x(i).
  θj:=θj−α1m∑mi=1(θTx−y(i))x(i)j (simultaneously update for all j).
  θ:=θ−α1m∑mi=1(11+e−θTx(i)−y(i))x(i).

4. Question 4
Which of the following statements are true? Check all that apply.
f  Linear regression always works well for classification if you classify by using a threshold on the prediction made by linear regression.

------------------



cost function of 1/2 squared error when using the sigmoid function, will be non-convex, wavy, with many local optima
instead we use 
cost function with logs and and conditional y=1 and y=0
------------------

1 and 5 correct


5. I chose the first one.


1. Question 1
Suppose that you have trained a logistic regression classifier, and it outputs on a new example x a prediction hθ(x) = 0.4. 
This means (check all that apply):

  Our estimate for P(y=0|x;θ) is 0.4.

x Our estimate for P(y=1|x;θ) is 0.4.

  Our estimate for P(y=1|x;θ) is 0.6.

x Our estimate for P(y=0|x;θ) is 0.6.


2. Question 2
Suppose you have the following training set, and fit a logistic regression classifier hθ(x)=g(θ0+θ1x1+θ2x2).
Which of the following are true? Check all that apply.

*  J(θ) will be a convex function, so gradient descent should converge to the global minimum.

*  Adding polynomial features (e.g., instead using hθ(x)=g(θ0+θ1x1+θ2x2+θ3x21+θ4x1x2+θ5x22) ) could increase how well we can fit the training data.

f  The positive and negative examples cannot be separated using a straight line. So, gradient descent will fail to converge.

f  Because the positive and negative examples cannot be separated using a straight line, linear regression will perform as well as logistic regression on this data.


3. Question 3
For logistic regression, the gradient is given by ∂∂θjJ(θ)=1m∑mi=1(hθ(x(i))−y(i))x(i)j. 
Which of these is a correct gradient descent update for logistic regression with a learning rate of α? Check all that apply.

f  θ:=θ−α1m∑mi=1(θTx−y(i))x(i).

*  θ:=θ−α1m∑mi=1(hθ(x(i))−y(i))x(i).

f  θj:=θj−α1m∑mi=1(θTx−y(i))x(i)j (simultaneously update for all j).

f  θ:=θ−α1m∑mi=1(11+e−θTx(i)−y(i))x(i).


4. Question 4
Which of the following statements are true? Check all that apply.

f  For logistic regression, sometimes gradient descent will converge to a local minimum (and fail to find the global minimum). This is the reason we prefer more advanced optimization algorithms such as fminunc (conjugate gradient/BFGS/L-BFGS/etc).

f  Linear regression always works well for classification if you classify by using a threshold on the prediction made by linear regression.

* The sigmoid function g(z)=1/1+e−z is never greater than one (>1).

* The cost function J(θ) for logistic regression trained with m≥1 examples is always greater than or equal to zero.


5. Suppose you train a logistic classifier hθ(x)=g(θ0+θ1x1+θ2x2). Suppose θ0=6,θ1=−1,θ2=0. Which of the following figures represents the decision boundary found by your classifier?

x first choice, y=1 when x1 less than 6

