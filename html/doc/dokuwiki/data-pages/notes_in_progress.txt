====== Notes in Progress ======

==== Resources to consume ====
  * <https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1//67000Dx//ZCJB-3pi>  3blue1brown youtube channel with numerous videos on math with animations\\
    * Neural Net Deep Learning 1, ft hot chick\\
    * Neural Net Deep Learning 2, done\\
    * Neural Net Deep Learning 3, Back Propagation, not public yet\\
  * <http://neuralnetworksanddeeplearning.com>  online html book\\
  * <https://www.deepbootcamp.io/>  website on Google DeepMind bootcamp on Reinforcement Learning
    * <https://www.youtube.com/watch?v=bsuvM1jO-4w&feature=youtu.be>  youtube from RL bootcamp
  * <http://colah.github.io/>  Christopher Ohlah's blog, multiple articles, visualizations, article on NHIST
  * <https://distill.pub>  Distill Blog, co-edited by Christopher Ohlah
  * https://www.tensorflow.org/get//started/get//started  tensorflow getting started, 1/3 of the way thru the first tutorial

==== great thoughts ====

TensorFlow and Caffe

Theano, python package for machine learning supported by Montreal Institute for Leaning Algorithms (MILA) from 2010 to 2017.  Eclipsed by TensorFlow and Caffe.

Caffe, python package for machine learning supported by Berkeley AI Research (BAIR).


Yoshua Bengio, Head of Montreal Institute for Leaning Algorithms (MILA)

Yangqing Jia (贾扬清), created Caffe at Berkeley, went to Google Brain and worked on TensorFlow, now at Facebook working on Caffe2 and ONNX.  [ONNX is the attempt by Facebook and Microsoft to keep themselves relevant under the onslaught of TensorFlow.]

equation drawing with rmarkdown, latex http://www.statpower.net/Content/310/R%20Stuff/SampleMarkdown.html\\

python is installed here\\
C:\Users\John\AppData\Local\Programs\Python\Python36\\

language  imprecision\\
In week 1, he uses the term "dimensional" incorrectly.
In week 2, Andrew Ng uses the word "multivariant linear regression" incorrectly

m = number of rows in the training data
n = number of independent variables in each row

independent variable - aka predictor, aka feature

there is no such thing as a 4-dimensional vector
a vector is a 1-dimensional matrix and a 1-dimensional tensor
this vector has 4 thingies occuring in one dimension

elements of an array
member of a set

vector in physics vs vector in matrix math

vector = array = a list of things, members, elements, numbers

with only two features, the cost function can be represented by a surface or contour

feature scaling = normalization
get every feature into
-1 < x < +1 or
-3 < x < +3 or
0 < x < +1  - this last is called normalization
-.5 < 0 < +.5 this is called mean normalization, where the mean value is zero



feature scaling improves the efficience of gradient descent
demonstrated by the 2 variable case, where if one variable has a range of 1 to 2000 and the other variable has a range of 1 to 5,
the resulting contour will be long and skinny
and gradient descent works much better if the countour is zero.
so the squared errors of each variable are roughly similar in scale to one another

if feature scaling is not done to the data before hand
it will be implicitly done by the weights
so by feature scaling up front, we give more meaning to the resulting weights

alpha = learning rate
try .003, .03, .3, 3
plot the curve of the cost function y, by number of iterations x
in should be the kind of curve that is an asymptote (a parabola that never reaches the 0 origin on either axis)


polynomial regression
using a curve instead of a line, square, cubic, quadratic, etc
same use of least squares and gradient descent
feature scaling becomes important, because of squares and cubes in the equation, which increase the artificial weight of each feature


what is the difference between regression and neural net?
in a neural net, each neuron implements a regression analysis


the normal equation
in the beginning Andrew Ng uses the notation X//{ij}, and later X^{(i)}//j for an element in a matrix


regression analysis
	linear regression
	non-linear regression
	fitting data to a line or curve
	least squares
	normal equation
	gradient descent


1809, Carl Frederich Gauss formalized the least squares method.  Many predecessors had worked through the 1700s refining the concept of minimizing the total error in observations vs predictions of planetary orbits.  The term "regression" was not used until the 1920s.
1920, The word "regression" was first used to refer to the process of using the least squares method to fit data to a line or curve.



https://en.wikipedia.org/wiki/Gallery//of//curves

a parabola has two x,s for every y
what about a curve that has two y's for every x?  the function f(x) would have two results for every x.  like a circle for example.
how do you graph a circle?  x^2 + y^2 = 0


if the data matches an algebraic curve
non-linear data can sometimes be solved with either linear or nonlinear regression
for the former, we fudge the data until it fits a straight line

linear
nonlinear
curvilinear

http://blog.minitab.com/blog/adventures-in-statistics-2/what-is-the-difference-between-linear-and-nonlinear-equations-in-regression-analysis
http://blog.minitab.com/blog/adventures-in-statistics-2/curve-fitting-with-linear-and-nonlinear-regression
https://en.wikipedia.org/wiki/Curvilinear_coordinates



---------------
x = 5184
range = max - min = 8836 - 4761 = 4075
mean = (7921+5184+8836+4761) / 4 = 6675.5
smean =
scaled = 5184/(8836 - 4761) = 1.2721472392638036
normalized = scaled-mean/range =

x - mean / range
5184 - 6675.5 / 4075

min
4761 / 4075
8836 / 4075

-----------


this is the Octave command to calculate theta using normal equation
\theta = pinv({X}'*X)*{X}'*y

feature scaling is NOT necessary in normal equation method

Sigmoid - old school, replaced by ReLU\\
ReLU  Rectified Linear Unit  function, straight horizontal line to a point then an angle up\\
source minute 17:00 of https://www.youtube.com/watch?v=aircAruvnKk\\

==== Notes   ====
Moreover, the holders of this much data remain in the hands of the private sector in the big six: Amazon, Facebook, Google, Microsoft, Apple and Baidu."\\
----\\
GAN Generative Adversarial Network, see Wikipedia\\
used to generate additional data.  more data helps the training.  "augmented data"\\
-----\\
Datasets\\
News\\
The web\\
A crawler\\
Output of a crawler, which links proved fruitful\\
Pandora music\\
\\
Build an AI that can create data sets\\
\\
Datasets to build\\
Map shapes\\
Treasures\\
Crawler - predict which links to follow\\
\\
Teach an AI to play plunder\\
Let the ai identify boring vs addictive game play\\
\\
How does ocr work? Pattern matching\\
\\
How to train s human\\
Human gets tired\\
Machine does not\\
Need efficient training techniques\\
\\
How is a glossary done in wiki pedia?\\
\\
Look at everything humans do and figure out how to do it by machine.\\
\\
Massage data into a training set.\\
Decide what to do, vs how to do it.\\
\\
Technology\\
Logic\\
Prayer\\
\\
Matrix\\
Variables × weights\\
\\
Vector, matrix. Now what is a tensorflow.\\
\\
----\\
\\
\\
https://www.technocracy.news/index.php/2017/08/27/147-transnational-companies-run-world/\\
\\
Find the Illuminati\\
\\
Draw the hierarchy of humans\\
Hold them to account\\
\\
Keep the population under control\\
Keep the environment conducive to human life\\
\\
Really? But what about the future life forms?\\
Attempt s to freeze a moment of happiness, to create a Utopia, backfire by repressing "natural" development.\\
\\
Nothing needs to be done. Everything is perfect. \\
Enjoy.\\
Be.\\
\\
----\\
\\
Use ai to geocode and chonocode, timestamp article s\\
to find historical maps, georeference, break into shapes, chronocode, timestamp, label\\
so that AGI can put human population movements into historical perspective\\
\\
AGI needs historical perspective.\\
Therefore, all data must be timestamped.\\
\\
all data must be timestamped and geocoded\\
\\
perhaps the AGI will be able to figure out why people tweet\\
\\
----\\
\\
Game-playing API.\\
Create multiple clones of itself.  Have the clones play against each other in leagues and tournaments.  Observe the different personalities and styles of play that develop.  Bring the  learning of all the clones back into itself, delete the clones, and go defeat the humans.\\
\\
Meta learning.  Use one neural network to create and test multiple neural networks to pick the best one.  https://arxiv.org/abs/1703.01041\\
\\
Sense of self\\
Ego\\
Metaphysics\\
\\
Tools\\
Tools become machines\\
Machines become intelligent\\
Child supersedes the father\\
\\
\\
Mimic the brain\\
\\
Mimic natural selection\\
\\
\\
